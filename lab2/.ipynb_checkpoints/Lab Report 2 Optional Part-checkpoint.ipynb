{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from scipy.misc import imresize  # resize images\n",
    "import copy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential functions\n",
    "def loadDataset(path, num_per_class, classes):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for id, class_name in classes.items():\n",
    "        img_path_class = glob.glob(path + class_name + '/*.jpg')\n",
    "        \n",
    "        if num_per_class > 0:\n",
    "            img_path_class = img_path_class[:num_per_class]\n",
    "        \n",
    "        labels.extend([id]*len(img_path_class))\n",
    "        \n",
    "        for filename in img_path_class:\n",
    "            data.append(cv2.imread(filename, 0))\n",
    "            \n",
    "    return data, labels\n",
    "\n",
    "def computeSIFT(data):\n",
    "    x = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(data))):\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        img = data[i]\n",
    "        step_size = 15\n",
    "        kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, img.shape[0], step_size)\n",
    "              for y in range(0, img.shape[1], step_size)]\n",
    "        dense_feat = sift.compute(img, kp)\n",
    "        x.append(dense_feat[1])\n",
    "        \n",
    "    return x\n",
    "\n",
    "def extract_denseSIFT(img):\n",
    "    DSIFT_STEP_SIZE = 2\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    disft_step_size = DSIFT_STEP_SIZE\n",
    "    keypoints = [cv2.KeyPoint(x, y, disft_step_size)\n",
    "            for y in range(0, img.shape[0], disft_step_size)\n",
    "                for x in range(0, img.shape[1], disft_step_size)]\n",
    "\n",
    "    descriptors = sift.compute(img, keypoints)[1]\n",
    "    \n",
    "    #keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return descriptors\n",
    "\n",
    "def SPM(L, img, kmeans, k):\n",
    "    W = img.shape[1]\n",
    "    H = img.shape[0]   \n",
    "    h = []\n",
    "    for l in (range(L+1)):\n",
    "        w_step = math.floor(W/(2**l))\n",
    "        h_step = math.floor(H/(2**l))\n",
    "        x, y = 0, 0\n",
    "        for i in range(1,2**l + 1):\n",
    "            x = 0\n",
    "            for j in range(1, 2**l + 1):                \n",
    "                desc = extract_denseSIFT(img[y:y+h_step, x:x+w_step])                \n",
    "                #print(\"type:\",desc is None, \"x:\",x,\"y:\",y, \"desc_size:\",desc is None)\n",
    "                predict = kmeans.predict(desc)\n",
    "                histo = np.bincount(predict, minlength=k).reshape(1,-1).ravel()\n",
    "                weight = 2**(l-L)\n",
    "                h.append(weight*histo)\n",
    "                x = x + w_step\n",
    "            y = y + h_step\n",
    "            \n",
    "    hist = np.array(h).ravel()\n",
    "    # normalize hist\n",
    "    dev = np.std(hist)\n",
    "    hist -= np.mean(hist)\n",
    "    hist /= dev\n",
    "    return hist\n",
    "\n",
    "# get histogram representation for training/testing data\n",
    "def getHistogramSPM(L, data, kmeans, k):    \n",
    "    x = []\n",
    "    for i in range(len(data)):        \n",
    "        hist = SPM(L, data[i], kmeans, k)        \n",
    "        x.append(hist)\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 102\n"
     ]
    }
   ],
   "source": [
    "class_names = [name.split('\\\\')[1] for name in glob.glob('101_ObjectCategories/*')]\n",
    "class_names = dict(zip(range(0,len(class_names)), class_names))\n",
    "\n",
    "# for key, value in class_names.items():\n",
    "#     print(key, ':', value)\n",
    "numOfClasses = len(class_names)\n",
    "print(\"Number of Classes:\", numOfClasses)\n",
    "\n",
    "# load training dataset\n",
    "data, label = loadDataset('101_ObjectCategories/*', 80, class_names)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size = 5/8, random_state = 42)\n",
    "print(f\"x shape: {len(X_train)} | y shape: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Pyramid Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa35bd8f6a5a420ea2f4e4699cb87efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2291.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99eb947fef8403e81fb0c80536bd1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3820.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract dense sift features from training images\n",
    "x_train_Computed = computeSIFT(X_train)\n",
    "x_test_Computed = computeSIFT(X_test)\n",
    "\n",
    "all_train_desc = []\n",
    "for i in tqdm(range(len(x_train_Computed))):\n",
    "    for j in range(x_train_Computed[i].shape[0]):\n",
    "        all_train_desc.append(x_train_Computed[i][j,:])\n",
    "\n",
    "all_train_descriptors = np.array(all_train_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(all_train_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_histo = getHistogramSPM(0, X_train, kmeans, k)\n",
    "test_histo = getHistogramSPM(0, X_test, kmeans, k)\n",
    "clf = LinearSVC(random_state=0, C=0.0005)\n",
    "clf.fit(train_histo, y_train)\n",
    "predict = clf.predict(test_histo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.76237623762376 %\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy:\", np.mean(predict == y_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lab2]",
   "language": "python",
   "name": "conda-env-lab2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
